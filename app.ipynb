{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import streamlit as st\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor  # ADDED: second ML model for final project\n",
    "\n",
    "from src.preprocessing import load_merge, sample_panel\n",
    "from src.utils import (\n",
    "    kpis_for_slice,\n",
    "    anomaly_flags,\n",
    "    seasonal_naive,\n",
    "    moving_avg,\n",
    "    safe_num,\n",
    ")\n",
    "\n",
    "# --------------------- Page config & Title ---------------------\n",
    "st.set_page_config(\n",
    "    page_title=\"Walmart Intelligence — Executive Sales, Pricing, Forecasting & Scenarios\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\",\n",
    ")\n",
    "st.title(\"Walmart Intelligence — Executive Sales, Pricing, Forecasting & Scenarios\")\n",
    "st.caption(\n",
    "    \"The purpose of this project is to create an end-to-end analytics dashboard that allows Walmart leadership to quickly understand sales performance, pricing behavior, operational patterns, and forecasted demand across stores and items. My goal is to take the raw Kaggle M5 dataset and transform it through the full data science workflow, including data cleaning, feature engineering, anomaly detection, forecasting models, and scenario simulations. I wanted to design a dashboard that organizes complex information into clear visuals so leadership can access insights instantly without reviewing code or raw data. The project aims to highlight the key factors driving performance, compare models with measurable metrics, and identify meaningful trends across different store and item segments. I also focused on making the tool actionable by incorporating interactive filters and scenario levers that support real decision-making. Through this approach, the dashboard becomes a streamlined, mobile-friendly resource that answers high-level business questions on demand.\"\n",
    ")\n",
    "\n",
    "# --------------------- Small helpers for page descriptions ---------------------\n",
    "def tab_intro(title: str, what: str, why: str, how: str, read: str = \"\"):\n",
    "    st.subheader(title)\n",
    "    with st.expander(title, expanded=True):\n",
    "        st.markdown(f\"**What this shows:** {what}\")\n",
    "        st.markdown(f\"**Why it matters:** {why}\")\n",
    "        st.markdown(f\"**How to use it:** {how}\")\n",
    "        if read:\n",
    "            st.caption(read)\n",
    "\n",
    "def describe_chart(header: str, meaning: str, action: str = \"\"):\n",
    "    st.markdown(f\"**About this visualization — {header}**\")\n",
    "    st.markdown(meaning)\n",
    "    if action:\n",
    "        st.caption(action)\n",
    "\n",
    "# --------------------- Load data ---------------------\n",
    "try:\n",
    "    base = load_merge(use_cache=True)  # reads the three ZIPs and returns a tidy daily panel\n",
    "except FileNotFoundError as e:\n",
    "    st.error(str(e))\n",
    "    st.stop()\n",
    "\n",
    "# --------------------- Sidebar controls ---------------------\n",
    "with st.sidebar:\n",
    "    st.header(\"Filters\")\n",
    "    n_stores = st.number_input(\"How many stores to include\", 1, 10, 3, step=1)\n",
    "    n_items = st.number_input(\"How many products per store\", 5, 200, 30, step=5)\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"Upload optional costs to enable profit tiles.\")\n",
    "    cost_file = st.file_uploader(\"costs.csv (store_id,item_id,unit_cost)\", type=[\"csv\"])\n",
    "\n",
    "# create a smaller working panel for snappier UI\n",
    "panel = sample_panel(base, int(n_stores), int(n_items))\n",
    "\n",
    "costs_df = None\n",
    "if cost_file is not None:\n",
    "    try:\n",
    "        tmp = pd.read_csv(cost_file)\n",
    "        if {\"store_id\", \"item_id\", \"unit_cost\"}.issubset(tmp.columns):\n",
    "            costs_df = tmp.copy()\n",
    "        else:\n",
    "            st.sidebar.warning(\"Expected columns: store_id, item_id, unit_cost\")\n",
    "    except Exception as e:\n",
    "        st.sidebar.warning(f\"Could not read costs.csv: {e}\")\n",
    "\n",
    "stores = sorted(panel[\"store_id\"].dropna().unique().tolist())\n",
    "store_sel = st.sidebar.selectbox(\"Store\", stores, index=0 if stores else None)\n",
    "items = sorted(panel.loc[panel[\"store_id\"] == store_sel, \"item_id\"].dropna().unique().tolist())\n",
    "item_sel = st.sidebar.selectbox(\"Product\", items, index=0 if items else None)\n",
    "\n",
    "dmin, dmax = pd.to_datetime(panel[\"date\"].min()), pd.to_datetime(panel[\"date\"].max())\n",
    "dr = st.sidebar.date_input(\n",
    "    \"Date range\",\n",
    "    (dmin.date(), dmax.date()),\n",
    "    min_value=dmin.date(),\n",
    "    max_value=dmax.date(),\n",
    ")\n",
    "start_ts = pd.Timestamp(dr[0])\n",
    "end_ts = pd.Timestamp(dr[1])\n",
    "\n",
    "slice_df = panel[\n",
    "    (panel[\"store_id\"] == store_sel)\n",
    "    & (panel[\"item_id\"] == item_sel)\n",
    "    & (panel[\"date\"].between(start_ts, end_ts))\n",
    "].copy()\n",
    "\n",
    "if costs_df is not None and not slice_df.empty:\n",
    "    slice_df = slice_df.merge(costs_df, on=[\"store_id\", \"item_id\"], how=\"left\")\n",
    "\n",
    "# --------------------- Analytics helpers ---------------------\n",
    "def estimate_elasticity(df_slice: pd.DataFrame) -> Optional[dict]:\n",
    "    g = df_slice.dropna(subset=[\"sell_price\", \"sales\"])\n",
    "    g = g[(g[\"sell_price\"] > 0) & (g[\"sales\"] > 0)]\n",
    "    if len(g) < 30:\n",
    "        return None\n",
    "    X = np.log(g[[\"sell_price\"]].values)\n",
    "    y = np.log(g[\"sales\"].values)\n",
    "    m = LinearRegression().fit(X, y)\n",
    "    yhat = m.predict(X)\n",
    "    return {\n",
    "        \"elasticity\": float(m.coef_[0]),\n",
    "        \"intercept\": float(m.intercept_),\n",
    "        \"mape\": float(mean_absolute_percentage_error(y, yhat)),\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y, yhat))),\n",
    "        \"n\": int(len(g)),\n",
    "    }\n",
    "\n",
    "def simulate_revenue(df_slice: pd.DataFrame, elasticity: float, pct: float) -> dict:\n",
    "    base_qty = safe_num(df_slice[\"sales\"].mean())\n",
    "    base_price = safe_num(df_slice[\"sell_price\"].mean())\n",
    "    new_price = base_price * (1 + pct)\n",
    "    new_qty = base_qty * (1 + pct) ** elasticity\n",
    "    base_rev = base_qty * base_price\n",
    "    new_rev = new_qty * new_price\n",
    "    return {\n",
    "        \"new_price\": float(new_price),\n",
    "        \"new_qty\": float(new_qty),\n",
    "        \"delta_rev_pct\": float((new_rev - base_rev) / (base_rev + 1e-9) * 100),\n",
    "    }\n",
    "\n",
    "def profit_kpis(df_slice: pd.DataFrame) -> Tuple[Optional[float], Optional[float], Optional[float]]:\n",
    "    if \"unit_cost\" not in df_slice.columns or df_slice[\"unit_cost\"].isna().all():\n",
    "        return None, None, None\n",
    "    qty = safe_num(df_slice[\"sales\"].mean())\n",
    "    price = safe_num(df_slice[\"sell_price\"].mean())\n",
    "    cost = safe_num(df_slice[\"unit_cost\"].mean())\n",
    "    margin = price - cost\n",
    "    profit = margin * qty\n",
    "    margin_pct = (margin / (price + 1e-9)) * 100 if price > 0 else None\n",
    "    return float(profit), float(margin), float(margin_pct) if margin_pct is not None else None\n",
    "\n",
    "def concise_summary(sl: pd.DataFrame, elasticity_val: Optional[float]) -> str:\n",
    "    if sl.empty:\n",
    "        return \"No data in view. Adjust the date or product.\"\n",
    "    k = kpis_for_slice(sl)\n",
    "    parts = []\n",
    "    if k[\"wow_pct\"] > 3:\n",
    "        parts.append(f\"Sales up {k['wow_pct']:.1f}% vs last week.\")\n",
    "    elif k[\"wow_pct\"] < -3:\n",
    "        parts.append(f\"Sales down {abs(k['wow_pct']):.1f}% vs last week.\")\n",
    "    else:\n",
    "        parts.append(\"Sales are roughly flat week-over-week.\")\n",
    "    parts.append(f\"Avg price ${k['avg_price']:.2f}.\")\n",
    "    if elasticity_val is not None:\n",
    "        parts.append(\"Price-sensitive product.\" if elasticity_val < -1.0 else \"Demand relatively steady vs price.\")\n",
    "    if \"event_name_1\" in sl.columns:\n",
    "        recent = sl.loc[sl[\"event_name_1\"].notna(), \"event_name_1\"].tail(3).unique().tolist()\n",
    "        if recent:\n",
    "            parts.append(\"Recent events: \" + \", \".join(recent[:3]) + \".\")\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def add_event_overlays(fig: go.Figure, df_: pd.DataFrame) -> go.Figure:\n",
    "    if \"event_name_1\" not in df_.columns:\n",
    "        return fig\n",
    "    ev = df_.loc[df_[\"event_name_1\"].notna(), [\"date\", \"event_name_1\"]].dropna()\n",
    "    ymax = max(df_[\"sales\"].max() if \"sales\" in df_.columns else 0, 1)\n",
    "    for _, row in ev.iterrows():\n",
    "        fig.add_vrect(x0=row[\"date\"], x1=row[\"date\"], fillcolor=\"orange\", opacity=0.08, line_width=0)\n",
    "        fig.add_annotation(\n",
    "            x=row[\"date\"], y=ymax, text=str(row[\"event_name_1\"]), showarrow=False, yshift=18, font=dict(size=10, color=\"gray\")\n",
    "        )\n",
    "    return fig\n",
    "\n",
    "def compute_wow(group_df: pd.DataFrame, freq=\"W\") -> float:\n",
    "    # guard against duplicate dates\n",
    "    s = group_df.groupby(\"date\", as_index=True)[\"sales\"].sum().resample(freq).sum()\n",
    "    if len(s) < 2:\n",
    "        return 0.0\n",
    "    prev, curr = float(s.iloc[-2]), float(s.iloc[-1])\n",
    "    return float(100 * (curr - prev) / (prev + 1e-9))\n",
    "\n",
    "# ---------- IDA helpers (EDA utilities used on Tab 0) ----------\n",
    "def info_table(df_: pd.DataFrame) -> pd.DataFrame:\n",
    "    non_null = df_.notnull().sum()\n",
    "    out = pd.DataFrame({\n",
    "        \"column\": df_.columns,\n",
    "        \"dtype\": [str(t) for t in df_.dtypes],\n",
    "        \"non_null\": [int(non_null[c]) for c in df_.columns],\n",
    "        \"nulls\": [int(len(df_) - non_null[c]) for c in df_.columns],\n",
    "    })\n",
    "    out[\"null_pct\"] = (out[\"nulls\"] / max(1, len(df_))).round(4)\n",
    "    return out\n",
    "\n",
    "def missingness_bar(df_: pd.DataFrame):\n",
    "    nn = df_.isnull().mean().sort_values(ascending=False).reset_index()\n",
    "    nn.columns = [\"column\", \"null_fraction\"]\n",
    "    fig = px.bar(nn, x=\"column\", y=\"null_fraction\", title=\"Missingness by Column\")\n",
    "    fig.update_layout(xaxis_tickangle=-45, yaxis_tickformat=\".0%\")\n",
    "    return fig\n",
    "\n",
    "def missingness_heatmap(df_: pd.DataFrame, sample_rows: int = 1200):\n",
    "    smp = df_.sample(min(sample_rows, len(df_)), random_state=42).isnull().astype(int)\n",
    "    fig = px.imshow(\n",
    "        smp.T,\n",
    "        color_continuous_scale=\"Blues\",\n",
    "        labels=dict(x=\"Sampled Row\", y=\"Column\", color=\"Is Null\"),\n",
    "        title=\"Missingness Heatmap (sampled rows)\",\n",
    "        aspect=\"auto\",\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def corr_heatmap(df_: pd.DataFrame, cols: List[str] | None = None):\n",
    "    if cols is None:\n",
    "        cols = [c for c in [\"sales\", \"sell_price\", \"wm_yr_wk\", \"snap\"] if c in df_.columns]\n",
    "        extra = [c for c in df_.select_dtypes(include=\"number\").columns if c not in cols]\n",
    "        cols += extra[:8]\n",
    "    num = df_[cols].select_dtypes(include=\"number\")\n",
    "    if num.empty or num.shape[1] < 2:\n",
    "        return None\n",
    "    mat = num.corr().round(3)\n",
    "    fig = px.imshow(mat, text_auto=True, aspect=\"auto\", title=\"Correlation Matrix (numeric features)\")\n",
    "    return fig\n",
    "\n",
    "# --------------------- Tabs ---------------------\n",
    "tabs = st.tabs([\n",
    "    \"IDA + Preprocessing\",\n",
    "    \"Overview\",\n",
    "    \"Forecast\",\n",
    "    \"Price Sensitivity\",\n",
    "    \"Scenario Compare\",\n",
    "    \"Compare Segments\",\n",
    "    \"Top Performers\",\n",
    "    \"Summary Export\",\n",
    "])\n",
    "\n",
    "# ===================== Tab 0: IDA + Preprocessing =====================\n",
    "with tabs[0]:\n",
    "    tab_intro(\n",
    "        title=\"IDA + Preprocessing\",\n",
    "        what=\"Schema, completeness, distributions, correlation matrix, and the exact cleaning used to build the daily panel.\",\n",
    "        why=\"Builds trust in the data and explains transformations that downstream tabs depend on (daily alignment, price carry-fill).\",\n",
    "        how=\"Scan structure & missingness, then the correlation matrix for expected relationships (e.g., price vs sales).\",\n",
    "        read=\"Data source: three CSVs shipped as ZIPs (calendar, prices, sales).\",\n",
    "    )\n",
    "\n",
    "    c1, c2, c3 = st.columns(3)\n",
    "    c1.metric(\"Rows in sampled panel\", f\"{len(panel):,}\")\n",
    "    c2.metric(\"Columns\", f\"{panel.shape[1]}\")\n",
    "    c3.metric(\"Date coverage\", f\"{dmin.date()} → {dmax.date()}\")\n",
    "\n",
    "    st.markdown(\"#### 1) Structure\")\n",
    "    st.dataframe(info_table(panel), use_container_width=True)\n",
    "    describe_chart(\n",
    "        \"Structure table\",\n",
    "        \"Types, non-null counts, and where gaps exist. Event fields are sparse by design; price/sales should be largely populated.\",\n",
    "        \"If key fields are mostly null for a segment, avoid heavy modeling on that segment until data improves.\",\n",
    "    )\n",
    "\n",
    "    st.markdown(\"#### 2) Numeric summary\")\n",
    "    num_cols = panel.select_dtypes(include=\"number\").columns\n",
    "    if len(num_cols) > 0:\n",
    "        st.dataframe(panel[num_cols].describe().T.round(3), use_container_width=True)\n",
    "    else:\n",
    "        st.info(\"No numeric columns found.\")\n",
    "    describe_chart(\n",
    "        \"Numeric summary\",\n",
    "        \"This table is the statistical summary for all numeric features. It shows count, mean, standard deviation, min, quartiles, and max for each variable.\",\n",
    "        \"Large spreads imply higher forecast uncertainty; compare mean vs median to check skew and look at extreme min / max values for possible outliers.\",\n",
    "    )\n",
    "\n",
    "    st.markdown(\"#### 3) Missingness\")\n",
    "    cA, cB = st.columns(2)\n",
    "    with cA:\n",
    "        st.plotly_chart(missingness_bar(panel), use_container_width=True)\n",
    "    with cB:\n",
    "        st.plotly_chart(missingness_heatmap(panel), use_container_width=True)\n",
    "    describe_chart(\n",
    "        \"Missingness views\",\n",
    "        \"The bar ranks columns by null fraction; the heatmap shows if missingness clusters. Narrative event text is naturally sparse.\",\n",
    "        \"Residual gaps in price are handled via within-item forward/back fill to make daily alignment reliable.\",\n",
    "    )\n",
    "\n",
    "    st.markdown(\"#### 4) Correlation matrix\")\n",
    "    corr_fig = corr_heatmap(panel)\n",
    "    if corr_fig is not None:\n",
    "        st.plotly_chart(corr_fig, use_container_width=True)\n",
    "    else:\n",
    "        st.info(\"Not enough numeric columns to compute a correlation matrix.\")\n",
    "    describe_chart(\n",
    "        \"Correlation matrix\",\n",
    "        \"Shows linear relationships among numeric features (for example, sales vs price, sales vs SNAP). A negative price–sales correlation is expected for elastic products.\",\n",
    "        \"Use it to sanity-check the elasticity direction for selected products and to understand which drivers move together.\",\n",
    "    )\n",
    "\n",
    "    st.markdown(\"#### 5) Behavioral signals\")\n",
    "    ts_all = panel.groupby(\"date\", as_index=False)[\"sales\"].sum()\n",
    "    st.plotly_chart(px.line(ts_all, x=\"date\", y=\"sales\", title=\"All-sampled items — daily sales\"), use_container_width=True)\n",
    "    describe_chart(\n",
    "        \"Daily sales timeline\",\n",
    "        \"Reveals baseline volatility and weekly rhythm; spikes often align with events, dips with outages or stockouts.\",\n",
    "        \"This justifies a weekly seasonal baseline in the Forecast tab.\",\n",
    "    )\n",
    "\n",
    "    st.markdown(\"#### 6) Imputation diagnostics (sell_price)\")\n",
    "    price_missing_after = float(panel[\"sell_price\"].isna().mean()) if \"sell_price\" in panel.columns else 1.0\n",
    "    st.caption(f\"Price still missing after within-item carry-fill: {price_missing_after:.1%}\")\n",
    "    samp = panel[[\"item_id\", \"sell_price\"]].dropna()\n",
    "    if not samp.empty:\n",
    "        samp = samp.sample(min(20000, len(samp)), random_state=42)\n",
    "        st.plotly_chart(\n",
    "            px.violin(\n",
    "                samp,\n",
    "                y=\"sell_price\",\n",
    "                points=False,\n",
    "                box=True,\n",
    "                title=\"Filled price distribution\",\n",
    "            ),\n",
    "            use_container_width=True,\n",
    "        )\n",
    "    describe_chart(\n",
    "        \"Price distribution after fill\",\n",
    "        \"This plot summarizes the distribution of `sell_price` **after** imputation. Prices inside each store–item pair are forward- and backward-filled so that every day has a price when possible.\",\n",
    "        \"Only price is imputed; unit sales remain as observed. Extreme outliers in price can be flagged for future capping before elasticity analysis.\",\n",
    "    )\n",
    "\n",
    "    st.markdown(\"#### 7) Auto insights\")\n",
    "    insights = []\n",
    "    try:\n",
    "        dow_means = panel.assign(dow=panel[\"date\"].dt.day_name()).groupby(\"dow\", as_index=False)[\"sales\"].mean()\n",
    "        wknd = dow_means.query(\"dow in ['Saturday','Sunday']\")[\"sales\"].mean()\n",
    "        mid = dow_means.query(\"dow in ['Tuesday','Wednesday','Thursday']\")[\"sales\"].mean()\n",
    "        if np.isfinite(wknd) and np.isfinite(mid) and mid > 0:\n",
    "            uplift = 100 * (wknd - mid) / (mid + 1e-9)\n",
    "            if abs(uplift) >= 5:\n",
    "                insights.append(\n",
    "                    f\"Weekly pattern present: weekend vs mid-week ≈ {uplift:+.1f}%. Supports a weekly seasonal baseline.\"\n",
    "                )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if \"event_name_1\" in panel.columns:\n",
    "        with_ev = panel.loc[panel[\"event_name_1\"].notna(), \"sales\"].mean()\n",
    "        no_ev = panel.loc[panel[\"event_name_1\"].isna(), \"sales\"].mean()\n",
    "        if np.isfinite(with_ev) and np.isfinite(no_ev) and no_ev > 0:\n",
    "            lift = 100 * (with_ev - no_ev) / no_ev\n",
    "            if abs(lift) >= 3:\n",
    "                insights.append(\n",
    "                    f\"Events relate to demand: event days differ by ≈ {lift:+.1f}% vs non-event days.\"\n",
    "                )\n",
    "\n",
    "    if \"snap\" in panel.columns and panel[\"snap\"].nunique() > 1:\n",
    "        snap_yes = panel.loc[panel[\"snap\"] == 1, \"sales\"].mean()\n",
    "        snap_no = panel.loc[panel[\"snap\"] == 0, \"sales\"].mean()\n",
    "        if np.isfinite(snap_yes) and np.isfinite(snap_no) and snap_no > 0:\n",
    "            snap_lift = 100 * (snap_yes - snap_no) / snap_no\n",
    "            insights.append(\n",
    "                f\"SNAP day signal present: ≈ {snap_lift:+.1f}% difference on average (varies by store/item).\"\n",
    "            )\n",
    "\n",
    "    # rough count of elasticity-ready pairs\n",
    "    ready = 0\n",
    "    for (_s, _i), g in panel.groupby([\"store_id\", \"item_id\"]):\n",
    "        g2 = g.dropna(subset=[\"sell_price\", \"sales\"])\n",
    "        g2 = g2[(g2[\"sell_price\"] > 0) & (g2[\"sales\"] > 0)]\n",
    "        if len(g2) >= 30:\n",
    "            ready += 1\n",
    "    insights.append(\n",
    "        f\"Elasticity can be estimated for ~{ready} product/store pairs in the sampled panel (≥ 30 valid days).\"\n",
    "    )\n",
    "\n",
    "    if insights:\n",
    "        st.markdown(\"- \" + \"\\n- \".join(insights))\n",
    "\n",
    "    # --------- 8) Explicit data collection + encoding + imputation narrative ----------\n",
    "    st.markdown(\"### 8) How data collection, encoding, and imputation are done in this project\")\n",
    "\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "**Data collection and dataset combination**\n",
    "\n",
    "This dashboard explicitly combines **three raw CSV files from the Walmart M5 dataset** into one unified daily panel:\n",
    "\n",
    "- `sales_train_validation.csv`: daily unit sales for each `store_id` × `item_id` combination, originally in wide `d_1 ... d_N` format.  \n",
    "- `calendar.csv`: maps each `d_` column to a real calendar `date`, `wm_yr_wk` (Walmart week), SNAP flags, and event descriptions.  \n",
    "- `sell_prices.csv`: weekly `sell_price` by `store_id`, `item_id`, and `wm_yr_wk`.\n",
    "\n",
    "In the preprocessing step (`load_merge` in `src.preprocessing`), these are merged in the following way:\n",
    "\n",
    "1. The sales file is **unpivoted** from wide (`d_1 ... d_N`) to long format so that each row is a single day of sales for one product in one store.  \n",
    "2. The long sales table is **joined to `calendar.csv`** using the day key (`d` → `date`, `wm_yr_wk`, events, SNAP, etc.).  \n",
    "3. The result is **joined to `sell_prices.csv`** on `store_id`, `item_id`, and `wm_yr_wk` so that each store–item–date row has the correct weekly price.  \n",
    "4. For each `store_id` × `item_id` pair, the code enforces a **dense daily index**, so the final panel contains a continuous time-series of days even when some prices are missing.\n",
    "\n",
    "This is the multi-source integration pipeline referred to in the project description and rubric.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "**Encoding and feature engineering**\n",
    "\n",
    "Several variables are encoded or engineered to make them usable for modeling and visualization:\n",
    "\n",
    "- **SNAP:** multiple state-specific SNAP flags from `calendar.csv` are collapsed into a single integer `snap` indicator (1 = SNAP active for that store's state, 0 = not active).  \n",
    "- **Events:** event names and types (for example, `event_name_1`, `event_type_1`) are kept as categorical text features and are used for overlays and group comparisons rather than one-hot encoded columns to keep storage low.  \n",
    "- **Date features:** from the `date` column we can derive day-of-week, week-of-year, and similar calendar encodings when needed (for example, the day-of-week calculation used in the auto-insights section above).  \n",
    "- **Identifiers:** `store_id`, `item_id`, `dept_id`, and `cat_id` remain as encoded categorical keys that drive filtering, grouping, and ranking in later tabs.\n",
    "\n",
    "This section explicitly documents the **encoding choices** that were applied after the raw CSVs were combined.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "**Imputation strategy**\n",
    "\n",
    "The project uses a **targeted imputation approach** focused on price:\n",
    "\n",
    "- For each `store_id` × `item_id` pair, missing `sell_price` values are filled using forward-fill and backward-fill within that product’s own history. This creates a stable, dense price series suitable for elasticity estimation and forecasting.  \n",
    "- Event fields and SNAP indicators are **not imputed**; when an event is missing, it is treated as \"no special event.\"  \n",
    "- Unit sales (`sales`) are **left as observed** without imputation, to avoid fabricating demand. Any structural zeros or gaps remain visible in the plots and summaries.\n",
    "\n",
    "The violin plot above is used as an **imputation diagnostic** to show the distribution of filled prices and to confirm that the carry-fill produced realistic values.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "# ===================== Tab 1: Overview =====================\n",
    "with tabs[1]:\n",
    "    tab_intro(\n",
    "        title=f\"Overview — {store_sel} / {item_sel}\",\n",
    "        what=\"Key KPIs for the selected product and store, plus a daily sales line with unusual days and any event overlays.\",\n",
    "        why=\"Gets everyone on the same page quickly—level, trend, volatility, and notable days.\",\n",
    "        how=\"Pick a store, product, and date range in the left panel. Hover the line to read event labels.\",\n",
    "    )\n",
    "    if slice_df.empty:\n",
    "        st.info(\"No data in the selected range.\")\n",
    "    else:\n",
    "        k = kpis_for_slice(slice_df)\n",
    "        profit_val, margin_val, margin_pct = profit_kpis(slice_df)\n",
    "        c1, c2, c3, c4 = st.columns(4)\n",
    "        c1.metric(\"Avg Daily Units\", f\"{k['avg_units']:.1f}\")\n",
    "        c2.metric(\"Avg Price\", f\"${k['avg_price']:.2f}\")\n",
    "        c3.metric(\"Week-over-Week\", f\"{k['wow_pct']:+.1f}%\")\n",
    "        if profit_val is not None:\n",
    "            c4.metric(\"Avg Daily Profit\", f\"${profit_val:,.2f}\")\n",
    "        else:\n",
    "            c4.metric(\"Days\", f\"{len(slice_df):,}\")\n",
    "\n",
    "        if profit_val is not None:\n",
    "            s1, s2 = st.columns(2)\n",
    "            s1.metric(\"Avg Margin ($/unit)\", f\"${margin_val:,.2f}\")\n",
    "            s2.metric(\"Avg Margin (%)\", f\"{margin_pct:.1f}%\" if margin_pct is not None else \"—\")\n",
    "        else:\n",
    "            st.caption(\"Upload costs.csv to enable profit and margin tiles.\")\n",
    "\n",
    "        flagged = anomaly_flags(slice_df, window=7, z=3.0)\n",
    "        fig = px.line(flagged, x=\"date\", y=\"sales\", labels={\"date\": \"Date\", \"sales\": \"Units\"})\n",
    "        if flagged[\"anomaly\"].sum() > 0:\n",
    "            a = flagged[flagged[\"anomaly\"] == 1]\n",
    "            fig.add_scatter(x=a[\"date\"], y=a[\"sales\"], mode=\"markers\", name=\"Unusual day\")\n",
    "        fig = add_event_overlays(fig, slice_df)\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "        describe_chart(\n",
    "            \"Daily sales with flags\",\n",
    "            \"Highlights unusual days that may be caused by promotions, outages, or data issues. Events are annotated when available.\",\n",
    "        )\n",
    "\n",
    "        elas = estimate_elasticity(slice_df)\n",
    "        st.info(concise_summary(slice_df, elas[\"elasticity\"] if elas else None))\n",
    "\n",
    "# ===================== Tab 2: Forecast =====================\n",
    "with tabs[2]:\n",
    "    tab_intro(\n",
    "        title=\"Forecast — next 4 weeks\",\n",
    "        what=\"Two simple 4-week baselines: one respects weekly seasonality; one smooths recent trend.\",\n",
    "        why=\"Fast planning signal for inventory and staffing when a heavy model isn’t necessary.\",\n",
    "        how=\"Pick ≥ 30 days of history. Compare the two curves to judge direction and rough magnitude.\",\n",
    "        read=\"For stronger accuracy, layer promotions and external drivers in a future version.\",\n",
    "    )\n",
    "    if slice_df.empty:\n",
    "        st.info(\"No data in the selected range.\")\n",
    "    else:\n",
    "        # guard: aggregate per calendar date to avoid duplicate index errors\n",
    "        series = (\n",
    "            slice_df.groupby(\"date\", as_index=True)[\"sales\"]\n",
    "            .sum()\n",
    "            .asfreq(\"D\")\n",
    "            .fillna(0.0)\n",
    "        )\n",
    "        if len(series) < 30:\n",
    "            st.warning(\"Select a longer range (30+ days) for a steadier forecast.\")\n",
    "        else:\n",
    "            fut_idx = pd.date_range(series.index[-1] + pd.Timedelta(days=1), periods=28, freq=\"D\")\n",
    "            fc1 = seasonal_naive(series, horizon=28, season=7)\n",
    "            fc2 = moving_avg(series, horizon=28, window=7)\n",
    "            fig = px.line(x=series.index, y=series.values, labels={\"x\": \"Date\", \"y\": \"Units\"})\n",
    "            fig.add_scatter(x=fut_idx, y=fc1, name=\"Season-aware (weekly)\")\n",
    "            fig.add_scatter(x=fut_idx, y=fc2, name=\"Smoothed trend\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            describe_chart(\n",
    "                \"Baseline forecasts\",\n",
    "                \"Weekly seasonal baseline mirrors the recent weekly cycle; moving average smooths short-term noise.\",\n",
    "                \"Use both as bookends; reality will vary with price and events.\",\n",
    "            )\n",
    "\n",
    "            # ---- ADDED: Final-project model-based forecast comparison (feature engineering + 2 ML models) ----\n",
    "            st.markdown(\"#### Model-based forecast comparison (final project)\")\n",
    "\n",
    "            if len(series) >= 60:\n",
    "                # Feature engineering: lag features and day-of-week\n",
    "                df_model = series.to_frame(name=\"sales\").copy()\n",
    "                for lag in range(1, 8):\n",
    "                    df_model[f\"lag_{lag}\"] = df_model[\"sales\"].shift(lag)\n",
    "                df_model[\"dow\"] = df_model.index.dayofweek  # 0=Mon,...,6=Sun\n",
    "\n",
    "                df_model = df_model.dropna()\n",
    "                feature_cols = [c for c in df_model.columns if c != \"sales\"]\n",
    "                X = df_model[feature_cols].values\n",
    "                y = df_model[\"sales\"].values\n",
    "\n",
    "                # Hold out the last 28 days for evaluation\n",
    "                if len(df_model) > 56:\n",
    "                    split = -28\n",
    "                else:\n",
    "                    split = int(len(df_model) * 0.8)\n",
    "\n",
    "                X_train, X_test = X[:split], X[split:]\n",
    "                y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "                # Model 1: LinearRegression on engineered features\n",
    "                lr_model = LinearRegression().fit(X_train, y_train)\n",
    "                preds_lr = lr_model.predict(X_test)\n",
    "\n",
    "                # Model 2: RandomForestRegressor on same features\n",
    "                rf_model = RandomForestRegressor(\n",
    "                    n_estimators=150,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1,\n",
    "                ).fit(X_train, y_train)\n",
    "                preds_rf = rf_model.predict(X_test)\n",
    "\n",
    "                rmse_lr = float(np.sqrt(mean_squared_error(y_test, preds_lr)))\n",
    "                rmse_rf = float(np.sqrt(mean_squared_error(y_test, preds_rf)))\n",
    "                mape_lr = float(mean_absolute_percentage_error(y_test, preds_lr) * 100)\n",
    "                mape_rf = float(mean_absolute_percentage_error(y_test, preds_rf) * 100)\n",
    "\n",
    "                metrics = pd.DataFrame(\n",
    "                    {\n",
    "                        \"Model\": [\n",
    "                            \"LinearRegression (lags + day-of-week)\",\n",
    "                            \"RandomForest (lags + day-of-week)\",\n",
    "                        ],\n",
    "                        \"RMSE\": [rmse_lr, rmse_rf],\n",
    "                        \"MAPE %\": [mape_lr, mape_rf],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                st.dataframe(\n",
    "                    metrics.style.format({\"RMSE\": \"{:.2f}\", \"MAPE %\": \"{:.1f}\"}),\n",
    "                    use_container_width=True,\n",
    "                )\n",
    "\n",
    "                # Plot actual vs predictions on the holdout window\n",
    "                dates_test = df_model.index[split:]\n",
    "                fig_models = px.line()\n",
    "                fig_models.add_scatter(\n",
    "                    x=dates_test,\n",
    "                    y=y_test,\n",
    "                    name=\"Actual (holdout)\",\n",
    "                )\n",
    "                fig_models.add_scatter(\n",
    "                    x=dates_test,\n",
    "                    y=preds_lr,\n",
    "                    name=\"LinearRegression (pred)\",\n",
    "                )\n",
    "                fig_models.add_scatter(\n",
    "                    x=dates_test,\n",
    "                    y=preds_rf,\n",
    "                    name=\"RandomForest (pred)\",\n",
    "                )\n",
    "                fig_models.update_layout(\n",
    "                    xaxis_title=\"Date\",\n",
    "                    yaxis_title=\"Units\",\n",
    "                    title=\"Model comparison on holdout window\",\n",
    "                )\n",
    "                st.plotly_chart(fig_models, use_container_width=True)\n",
    "\n",
    "                describe_chart(\n",
    "                    \"Model-based forecast comparison\",\n",
    "                    \"Two supervised models are fit on engineered features (lags and day-of-week). The table summarizes RMSE and MAPE on the holdout window, and the line chart shows how well each model tracks actual demand.\",\n",
    "                    \"Use this section to demonstrate model quality against a concrete test period and to justify which model would be used for future production forecasts.\",\n",
    "                )\n",
    "            else:\n",
    "                st.caption(\"Not enough history yet for a stable supervised model comparison (needs ≈ 60+ days).\")\n",
    "\n",
    "# ===================== Tab 3: Price Sensitivity =====================\n",
    "with tabs[3]:\n",
    "    tab_intro(\n",
    "        title=\"Price Sensitivity (elasticity)\",\n",
    "        what=\"Elasticity estimates how much units change (%) for a 1% price change, plus a peer comparison histogram.\",\n",
    "        why=\"Helps decide whether to prioritize volume (elastic) or margin (inelastic).\",\n",
    "        how=\"If there’s enough price variation, you’ll see an elasticity for the selected product; the histogram shows peer context.\",\n",
    "    )\n",
    "    if slice_df.empty:\n",
    "        st.info(\"No data in the selected range.\")\n",
    "    else:\n",
    "        elas = estimate_elasticity(slice_df)\n",
    "        if not elas:\n",
    "            st.info(\"Insufficient price variation/history (needs ~30+ valid days).\")\n",
    "        else:\n",
    "            st.success(f\"Elasticity: {elas['elasticity']:.2f} (n={elas['n']}, MAPE={elas['mape']:.3f})\")\n",
    "            vals = []\n",
    "            for (_s, _i), g in panel.groupby([\"store_id\", \"item_id\"]):\n",
    "                e = estimate_elasticity(g)\n",
    "                if e:\n",
    "                    vals.append(e[\"elasticity\"])\n",
    "            if vals:\n",
    "                h = pd.Series(vals).clip(-5, 5)\n",
    "                fig = px.histogram(h, nbins=40, labels={\"value\": \"Elasticity\", \"count\": \"Products\"})\n",
    "                fig.add_vline(x=elas[\"elasticity\"], line_dash=\"dot\", line_color=\"red\")\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "                describe_chart(\n",
    "                    \"Elasticity distribution\",\n",
    "                    \"Shows where this SKU sits vs peers. Red line is the current item’s estimate.\",\n",
    "                )\n",
    "\n",
    "# ===================== Tab 4: Scenario Compare =====================\n",
    "with tabs[4]:\n",
    "    tab_intro(\n",
    "        title=\"Scenario Compare\",\n",
    "        what=\"Three price scenarios side-by-side with projected revenue index and a downloadable price plan.\",\n",
    "        why=\"Quickly tests direction and rough magnitude of price moves before execution.\",\n",
    "        how=\"Enter three price deltas (− for discounts, + for increases). Review the bars and download the recommended plan.\",\n",
    "    )\n",
    "    if slice_df.empty:\n",
    "        st.info(\"No data in the selected range.\")\n",
    "    else:\n",
    "        e = estimate_elasticity(slice_df)\n",
    "        if not e:\n",
    "            st.info(\"Price sensitivity not available for this product/date window.\")\n",
    "        else:\n",
    "            elas_val = e[\"elasticity\"]\n",
    "            c1, c2, c3 = st.columns(3)\n",
    "            s1 = c1.number_input(\"Scenario A (Δ Price %)\", -50, 50, -10)\n",
    "            s2 = c2.number_input(\"Scenario B (Δ Price %)\", -50, 50, 0)\n",
    "            s3 = c3.number_input(\"Scenario C (Δ Price %)\", -50, 50, 10)\n",
    "            scenarios = {\"A\": s1, \"B\": s2, \"C\": s3}\n",
    "            out = []\n",
    "            for name, pct in scenarios.items():\n",
    "                r = simulate_revenue(slice_df, elas_val, pct / 100.0)\n",
    "                out.append(\n",
    "                    {\n",
    "                        \"Scenario\": name,\n",
    "                        \"Δ Price %\": pct,\n",
    "                        \"New Price\": r[\"new_price\"],\n",
    "                        \"Est Units\": r[\"new_qty\"],\n",
    "                        \"Revenue Δ%\": r[\"delta_rev_pct\"],\n",
    "                        \"Revenue ($ idx)\": 1 + r[\"delta_rev_pct\"] / 100.0,\n",
    "                    }\n",
    "                )\n",
    "            table = pd.DataFrame(out)\n",
    "            st.dataframe(\n",
    "                table.style.format({\"New Price\": \"${:.2f}\", \"Est Units\": \"{:.1f}\", \"Revenue Δ%\": \"{:+.1f}\"}),\n",
    "                use_container_width=True,\n",
    "            )\n",
    "            fig = px.bar(\n",
    "                table,\n",
    "                x=\"Scenario\",\n",
    "                y=\"Revenue ($ idx)\",\n",
    "                text=table[\"Revenue Δ%\"].map(lambda v: f\"{v:+.1f}%\"),\n",
    "                labels={\"Revenue ($ idx)\": \"Revenue index\"},\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            describe_chart(\n",
    "                \"Scenario bars\",\n",
    "                \"Revenue index compares each scenario to current average price/units. Positive indicates gain vs baseline.\",\n",
    "                \"Use as a directional screen before operationalizing price changes.\",\n",
    "            )\n",
    "            best_row = max(out, key=lambda r: r[\"Revenue ($ idx)\"])\n",
    "            st.success(\n",
    "                f\"Best of these: Scenario {best_row['Scenario']} \"\n",
    "                f\"({best_row['Δ Price %']:+.0f}%), projected revenue {best_row['Revenue Δ%']:+.1f}%.\"\n",
    "            )\n",
    "\n",
    "            st.download_button(\n",
    "                \"Download price plan (CSV)\",\n",
    "                data=pd.DataFrame(\n",
    "                    {\n",
    "                        \"store_id\": [store_sel],\n",
    "                        \"item_id\": [item_sel],\n",
    "                        \"scenario\": [best_row[\"Scenario\"]],\n",
    "                        \"recommended_price_change_pct\": [best_row[\"Δ Price %\"]],\n",
    "                    }\n",
    "                ).to_csv(index=False).encode(),\n",
    "                file_name=\"price_plan.csv\",\n",
    "                mime=\"text/csv\",\n",
    "            )\n",
    "\n",
    "# ===================== Tab 5: Compare Segments =====================\n",
    "with tabs[5]:\n",
    "    tab_intro(\n",
    "        title=\"Compare Segments\",\n",
    "        what=\"Ranks stores, categories, or departments by total units or week-over-week growth for the selected period.\",\n",
    "        why=\"Quick way to see where performance is strongest or slipping across the footprint.\",\n",
    "        how=\"Pick a grouping and a metric, then scan the bar chart to spot leaders and laggards.\",\n",
    "    )\n",
    "    dim = st.selectbox(\"Compare by\", [\"store_id\", \"cat_id\", \"dept_id\"], index=0)\n",
    "    metric = st.selectbox(\"Metric\", [\"Total units\", \"Week-over-week %\"], index=0)\n",
    "    df_range = panel[panel[\"date\"].between(start_ts, end_ts)].copy()\n",
    "    if df_range.empty:\n",
    "        st.info(\"No data in the selected range.\")\n",
    "    else:\n",
    "        if metric == \"Total units\":\n",
    "            agg = df_range.groupby(dim, as_index=False)[\"sales\"].sum().rename(columns={\"sales\": \"total_units\"})\n",
    "            agg = agg.sort_values(\"total_units\", ascending=False).head(15)\n",
    "            fig = px.bar(agg, x=dim, y=\"total_units\", labels={\"total_units\": \"Units\"})\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            describe_chart(\"Total units by segment\", \"Identifies the biggest volume contributors in the selected window.\")\n",
    "        else:\n",
    "            rows = []\n",
    "            for gval, gdf in df_range.groupby(dim):\n",
    "                rows.append({\"segment\": gval, \"wow_pct\": compute_wow(gdf, freq=\"W\")})\n",
    "            agg = pd.DataFrame(rows).sort_values(\"wow_pct\", ascending=False).head(15)\n",
    "            fig = px.bar(agg, x=\"segment\", y=\"wow_pct\", labels={\"wow_pct\": \"WoW %\"})\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            describe_chart(\n",
    "                \"Week-over-week by segment\",\n",
    "                \"Flags emerging winners/decliners based on week aggregation. Use to triage deeper dives.\",\n",
    "            )\n",
    "\n",
    "# ===================== Tab 6: Top Performers =====================\n",
    "with tabs[6]:\n",
    "    tab_intro(\n",
    "        title=\"Top Performers in this Store\",\n",
    "        what=\"Lists the top week-over-week risers and decliners inside the selected store.\",\n",
    "        why=\"Quickly surfaces items that need attention (lean in or mitigate).\",\n",
    "        how=\"Use together with Price Sensitivity to decide if price moves could help or hurt trajectory.\",\n",
    "    )\n",
    "    srange = panel[(panel[\"store_id\"] == store_sel) & (panel[\"date\"].between(start_ts, end_ts))].copy()\n",
    "    if srange.empty:\n",
    "        st.info(\"No data in the selected range.\")\n",
    "    else:\n",
    "        movers = []\n",
    "        for it, g in srange.groupby(\"item_id\"):\n",
    "            movers.append({\"item_id\": it, \"WoW %\": compute_wow(g, \"W\"), \"Total units\": g[\"sales\"].sum()})\n",
    "        mv = pd.DataFrame(movers)\n",
    "        top_winners = mv.sort_values(\"WoW %\", ascending=False).head(10)\n",
    "        top_decliners = mv.sort_values(\"WoW %\", ascending=True).head(10)\n",
    "        c1, c2 = st.columns(2)\n",
    "        with c1:\n",
    "            st.markdown(\"**Week-over-week increases**\")\n",
    "            st.dataframe(top_winners, use_container_width=True)\n",
    "        with c2:\n",
    "            st.markdown(\"**Week-over-week declines**\")\n",
    "            st.dataframe(top_decliners, use_container_width=True)\n",
    "        describe_chart(\n",
    "            \"Winners & Decliners\",\n",
    "            \"Ranks SKUs by recent momentum. Use this with margins and elasticity to prioritize actions.\",\n",
    "        )\n",
    "\n",
    "# ===================== Tab 7: Summary Export =====================\n",
    "with tabs[7]:\n",
    "    tab_intro(\n",
    "        title=\"Summary Export\",\n",
    "        what=\"One-page markdown and CSV export of the key numbers for the current slice.\",\n",
    "        why=\"Quick sharing with stakeholders; pairs well with a price plan from Scenario Compare.\",\n",
    "        how=\"Pick your store, product, dates, then download.\",\n",
    "    )\n",
    "    if slice_df.empty:\n",
    "        st.info(\"No data in the selected range.\")\n",
    "    else:\n",
    "        k = kpis_for_slice(slice_df)\n",
    "        elas = estimate_elasticity(slice_df)\n",
    "        elasticity_text = f\"{elas['elasticity']:.2f}\" if elas else \"N/A\"\n",
    "        direction = \"rose\" if k[\"wow_pct\"] > 0 else \"fell\" if k[\"wow_pct\"] < 0 else \"held steady\"\n",
    "        if elas:\n",
    "            interpretation_bit = (\n",
    "                \"Price sensitivity is meaningful; price moves have a noticeable effect.\"\n",
    "                if abs(elas[\"elasticity\"]) > 1\n",
    "                else \"Demand is relatively steady against price changes.\"\n",
    "            )\n",
    "        else:\n",
    "            interpretation_bit = \"Not enough history to estimate price sensitivity yet.\"\n",
    "\n",
    "        summary_text = f\"\"\"\n",
    "### Sales Snapshot — {store_sel} / {item_sel}\n",
    "\n",
    "**Period:** {start_ts.date()} → {end_ts.date()}  \n",
    "**Avg Units / Day:** {k['avg_units']:.1f}  \n",
    "**Avg Price / Unit:** ${k['avg_price']:.2f}  \n",
    "**Week-over-Week Change:** {k['wow_pct']:+.1f}%  \n",
    "**Elasticity:** {elasticity_text}\n",
    "\n",
    "**Interpretation:**  \n",
    "Sales {direction} {abs(k['wow_pct']):.1f}% vs the prior week. {interpretation_bit}\n",
    "\"\"\"\n",
    "        st.markdown(summary_text)\n",
    "\n",
    "        # explicit reminder that the summary is built from the multi-CSV, encoded, and imputed panel\n",
    "        st.caption(\n",
    "            \"This summary is computed from the unified daily panel created by merging \"\n",
    "            \"`sales_train_validation.csv`, `calendar.csv`, and `sell_prices.csv`, \"\n",
    "            \"after the encoding and price-imputation steps documented on the 'IDA + Preprocessing' tab.\"\n",
    "        )\n",
    "\n",
    "        st.download_button(\n",
    "            \"Download Markdown Summary\",\n",
    "            data=summary_text.encode(),\n",
    "            file_name=f\"summary_{store_sel}_{item_sel}.md\",\n",
    "            mime=\"text/markdown\",\n",
    "        )\n",
    "\n",
    "        row = {\n",
    "            \"Store\": store_sel,\n",
    "            \"Item\": item_sel,\n",
    "            \"Period Start\": str(start_ts.date()),\n",
    "            \"Period End\": str(end_ts.date()),\n",
    "            \"Avg Units/Day\": k['avg_units'],\n",
    "            \"Avg Price/Unit\": k['avg_price'],\n",
    "            \"Week-over-Week %\": k['wow_pct'],\n",
    "            \"Elasticity\": elas[\"elasticity\"] if elas else None,\n",
    "        }\n",
    "        st.download_button(\n",
    "            \"Download Data (CSV)\",\n",
    "            data=pd.DataFrame([row]).to_csv(index=False).encode(),\n",
    "            file_name=f\"summary_{store_sel}_{item_sel}.csv\",\n",
    "            mime=\"text/csv\",\n",
    "        )\n",
    "        st.caption(\"Attach a price plan from Scenario Compare when sharing recommendations.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
